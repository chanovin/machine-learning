---
title: "Identifying Common Mistakes in Unilateral Dumbbell Biceps Curl"
author: "Vincent Chan"
date: "March 15, 2017"
output:
  md_document:
    variant: markdown_github
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Executive Summary

A unilateral dumbbell biceps curl was performed with multiple repititions across multiple subjects. Each rep is classified for correct execution or execution with one of four of the most common form errors. Data from five sensors placed on the subjects was collected. This data was reduced to instantaneous data only, discarding distribution data for each rep. Comparing machine learning techniques (stochastic gradient boosting, random forests, and linear discriminant analysis), we find that random forests offers the best predictions based on instantaneous motion sensor data. Random forests offers 99.5% out-of-sample accuracy; stochastic gradient boosting offers 98.6% out-of-sample accuracy; and linear discriminant analysis had only 70.6% out-of-sample accuracy. We then use the random forest model to predict the error class for twenty test observations.

## Background

Per Velloso, et al:
"[H]uman activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training."

Velloso, et al provide data for many repititions of a unilateral dumbbell biceps curl across multiple subjects. Each repitition also has a "window" observation, describing motion analysis for the entire rep. (These include kurtosis, skewness, max, min, and amplitude of the collected data.) Every repitition is classified for correct form (Class A) or for falling into one of four common form errors (Classes B through E); a trainer was used to evaluate correct execution of form errors or lack thereof, with no compound form errors included in the data.

We are given 20 instantaneous data points for which we must predict the form classification. The raw data include 19622 observations of 160 variables, including "window" summary data.

## Data Manipulation and Cleaning

The data are loaded directly from the internet. Because we must make our predictions based on instantaneous data, we must drop all "window" summary data. This includes rows for "window" data and columns that are missing values except for "window" rows. We will also drop data describing observations that we know do not influence form, including subject identification, row index, and timestamp.

```{r loaddata, cache=TRUE, echo=TRUE}
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
training <- training[which(training$new_window!="yes"),]
training <- subset(training,select=-c(user_name, raw_timestamp_part_1, raw_timestamp_part_2,
                                      cvtd_timestamp, new_window))
training <- training[,colSums(is.na(training)) != nrow(training)]
training <- subset(training,select=-c(kurtosis_roll_belt,kurtosis_picth_belt,kurtosis_yaw_belt,
                                      skewness_roll_belt,skewness_roll_belt.1,skewness_yaw_belt,
                                      max_yaw_belt,min_yaw_belt,amplitude_yaw_belt))
training <- subset(training,select=-c(kurtosis_roll_arm,
                                      kurtosis_picth_arm,kurtosis_yaw_arm,skewness_roll_arm,
                                      skewness_pitch_arm,skewness_yaw_arm))
training <- subset(training,select=-c(kurtosis_roll_dumbbell,
                                      kurtosis_picth_dumbbell,kurtosis_yaw_dumbbell,
                                      skewness_roll_dumbbell,skewness_pitch_dumbbell,
                                      skewness_yaw_dumbbell,max_yaw_dumbbell,min_yaw_dumbbell,
                                      amplitude_yaw_dumbbell))
training <-  subset(training,select=-c(kurtosis_roll_forearm,kurtosis_picth_forearm,
                                       kurtosis_yaw_forearm,skewness_roll_forearm,
                                       skewness_pitch_forearm,skewness_yaw_forearm,
                                       max_yaw_forearm,min_yaw_forearm, amplitude_yaw_forearm))
training <- subset(training,select=-X)
```

This relatively clean dataset includes 19216 instantaneous observations of 53 possible covariates plus our classification variable.

## Cross-validation: Preparation

Because of the large dataset, the training algorithms will take a fair amount of time to compute a model. In consideration of that issue, we will use the holdout method of cross validation, dividing the provided data set into two equal-sized training and validation sets. The relatively large size of the validation set should represent out-of-sample error fairly well.

For more critical models or smaller datasets (or where computation time is less important), it would be advantageous to apply a k-folds method of cross-validation. (Even a two-fold cross-validation should improve outcomes.)

```{r splitset, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
library(gbm)
library(AppliedPredictiveModeling)

set.seed(314159)
indexTrain = createDataPartition(training$classe, p = 1/2)[[1]]
trainset = training[ indexTrain,]
valset = training[-indexTrain,]
```

## Model Creation 

Using the `caret` package, we train three models using the stochastic gradient boosting, random forests, and linear discriminant analysis learning algorithms.

```{r debugerr, echo=FALSE}
trainset1 <- trainset
```

```{r modelcreation, cache=TRUE, echo=TRUE, results='hide'}
mod1 <- train(classe~.,data=trainset,method="gbm")
mod2 <- train(classe~.,data=trainset,method="rf")
mod3 <- train(classe~.,data=trainset,method="lda")
```

We should evaluate these three models for in-sample error as a diagnostic.

```{r predicttrain, echo=TRUE,message=FALSE,warning=FALSE}
predTr1 <- predict(mod1, trainset)
predTr2 <- predict(mod2, trainset)
predTr3 <- predict(mod3, trainset)
```
```{r insample, echo=TRUE}
table(predTr1,trainset$classe)
sum(predTr1==trainset$classe)/length(trainset$classe)
table(predTr2,trainset$classe)
sum(predTr2==trainset$classe)/length(trainset1$classe)
table(predTr3,trainset$classe)
sum(predTr3==trainset$classe)/length(trainset$classe)
```

We see that random forests outperforms both stochastic gradient boosting and linear discriminant analysis. With RF and GBM performing so well, with accuracy rates at or near 100%, we are prepared to continue with these two models.

We can investigate the model-building algorithms briefly for our top-two models.

### Random Forests

```{r rf}
plot(mod2, main="Random Forests, mtry vs accuracy")
mod2
```

For building our random forests model, three values for the tuning parameter, `mtry`, were used. Additional values of `mtry` might be a useful way to improve the accuracy of this model. Tuning `mtry` would be an especially good application for k-fold cross-validation.

### Stochastic Gradient Boosting

```{r gbm}
plot(mod1, main="Stochastic Gradient Boosting \n iterations and tree depth vs accuracy")
mod1
```

The stochastic gradient boosting model-building algorithm varied two tuning parameters, `interaction.depth` and `n.trees`. In-sample accuracy appeared to improve with increasing values for both of these parameters, but this trend could lead to overfitting. Tuning these variables would be another great application for k-folds cross-validation.


## Cross-validation: Out-of-sample Error

```{r outsample, echo=TRUE}
predval1 <- predict(mod1,valset)
predval2 <- predict(mod2, valset)
predval3 <- predict(mod3, valset)
table(predval1, valset$classe)
sum(predval1==valset$classe)/length(valset$classe)
table(predval2, valset$classe)
sum(predval2==valset$classe)/length(valset$classe)
table(predval3, valset$classe)
sum(predval3==valset$classe)/length(valset$classe)
```

As expected from our in-sample error values, the random forests model continues to outperform stochastic gradient boosting model. The linear discriminant analysis model lags behind. All out-of-sample error rates are higher than the in-sample error rates, but the difference is not so great as to suggest overfitting. (RF and GBM both have high out-of-sample accuracies, > 97.5%, which is great for a validation prediction.)

Given RF's outstanding performance here, as well, we choose it as the best model for prediction in our testing set.

## Testing/Prediction


Given the data for 20 observations, we predict the following form classes:

```{r predict, echo=TRUE}
predtest2 <- predict(mod2,testing)
predtest2
```


## References

Dataset provided by:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz4bPfDrafj
